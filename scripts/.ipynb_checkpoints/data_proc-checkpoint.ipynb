{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate policy diffusion \n",
    "- This script supports the analysis in the 'Climate policy diffusion' project. \n",
    "- This analysis seeks to uncover a link between the 'international environment' and the adoption of climate policy. In particular, it is interested in identifying (i) salient 'factors' and (ii) diffusion channels.\n",
    "- It is focused exclusively on policy adoption in the power sector of national jurisdictions\n",
    "- Each set of estimated models focuses on one specific policy:\n",
    "    - Emissions Trading Systems (pricing mechanism)\n",
    "    - Carbon Taxes (pricing mechanism)\n",
    "    - Feed in Tariffs\n",
    "    - Renewable Energy Quotas (non-price mechanism)\n",
    "    \n",
    "POLICY\n",
    "Bilateral trade could capture general bilateral economic integration - in which case bilat x policy ==> signal; (making a case for bilat trade to affect technology diffusion would require to work at a disaggregated level, at the very least)\n",
    "\n",
    "\"Bilateral\" patenting activity could capture technological diffusion;\n",
    "\n",
    "TECHNOLOGY\n",
    "It's about access to technology more than 'diffusion' per se - so one can test global vs 'weighted' technological pools\n",
    "Like nodes in a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pprint\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "\n",
    "import numpy.linalg as LA\n",
    "\n",
    "## Just making the plots look better\n",
    "mpl.style.use('ggplot')\n",
    "mpl.rcParams['figure.figsize'] = (8,6)\n",
    "mpl.rcParams['font.size'] = 12\n",
    "\n",
    "path_root = '/Users/GD/OneDrive - rff/Documents/Research/projects/climate_policy_diffusion/data/raw_data'\n",
    "path_root_ii = '/Users/GD/OneDrive - rff/Documents/Research/projects/climate_policy_diffusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(indir):#,outfile):\n",
    "    os.chdir(indir) #sets the current directory to 'indir'\n",
    "    fileList=glob.glob(\"*.csv\") #this command generates a list of csv files\n",
    "    dfList = []\n",
    "\n",
    "    #each iteration of the loop will add a dataframe to the list\n",
    "    for filename in fileList:\n",
    "        df=pd.read_csv(filename, header=0)\n",
    "        dfList.append(df)\n",
    "    concatDf=pd.concat(dfList,axis=0) #'axis=0' means that we are concatenating vertically, if we wanted to concatenate horizontally, we should use 'axis=1'\n",
    "\n",
    "    return concatDf\n",
    "\n",
    "map_iea_wb = {\"CÃ\\x83Â´te d'Ivoire\": \"Cote d'Ivoire\", \"CÃ´te d'Ivoire\": \"Cote d'Ivoire\",\n",
    "              '\"China (P.R. of China and Hong Kong, China)\"': 'China (P.R. of China and Hong Kong, China)',\n",
    "              \"People's Republic of China\": 'China', 'CuraÃ\\x83Â§ao/Netherlands Antilles': 'Curacao/Netherlands Antilles',\n",
    "              'CuraÃ§ao': 'Curacao', 'CuraÃ§ao/Netherlands Antilles': 'Curacao/Netherlands Antilles',\n",
    "              'Democratic Republic of Congo': 'Congo, Dem. Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.',\n",
    "              'Republic of the Congo': 'Congo, Rep.', 'Egypt': 'Egypt, Arab Rep.', 'Hong Kong (China)': 'Hong Kong SAR, China',\n",
    "              'Islamic Republic of Iran': 'Iran, Islamic Rep.', \"Democratic People's Republic of Korea\": 'Korea, Dem. Rep.',\n",
    "              'Korea': 'Korea, Rep.', 'Kyrgyzstan': 'Kyrgyz Republic', 'Republic of North Macedonia': 'North Macedonia',\n",
    "              'Republic of Moldova':'Moldova', 'Chinese Taipei':'Taiwan, China',\n",
    "              'Venezuela': 'Venezuela, RB', 'Plurinational State of Bolivia':'Bolivia',\n",
    "              'United Republic of Tanzania':'Tanzania',\n",
    "              'Bolivarian Republic of Venezuela': 'Venezuela, RB', 'Viet Nam': 'Vietnam', 'Yemen': 'Yemen, Rep.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Policy variables (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carbon pricing policies\n",
    "\n",
    "indir = \"/Users/gd/GitHub/WorldCarbonPricingDatabase/Data/national_jur\"\n",
    "cp = concatenate(indir)\n",
    "cp = cp[[\"Jurisdiction\", \"Year\", \"IEA_CODE\", \"Product\", \"Tax_dummy\", \"ETS_dummy\"]]\n",
    "\n",
    "# select power sector only\n",
    "cp = cp[cp[\"IEA_CODE\"].isin([\"ABFLOW003\"])]\n",
    "cp.drop(\"IEA_CODE\", axis=1, inplace=True)\n",
    "cp.rename(columns={\"Jurisdiction\":\"Country\"}, inplace=True)\n",
    "\n",
    "# aggregate across fuels (product)\n",
    "cp = cp.groupby(by=[\"Country\", \"Year\"]).sum()\n",
    "cp.reset_index(inplace=True)\n",
    "\n",
    "# set value to 1 if > 0\n",
    "cp.loc[cp[\"Tax_dummy\"] > 0, \"Tax_dummy\"] = 1\n",
    "cp.loc[cp[\"ETS_dummy\"] > 0, \"ETS_dummy\"] = 1\n",
    "\n",
    "\n",
    "#FiT and Renewable Portfolio Standards\n",
    "\n",
    "fit = pd.read_excel(path_root+\"/FiT_RPS/FiT_RPS_adoption.xlsx\", sheet_name=\"FiT\")\n",
    "rps = pd.read_excel(path_root+\"/FiT_RPS/FiT_RPS_adoption.xlsx\", sheet_name=\"RPS\")\n",
    "\n",
    "fit = fit.melt(id_vars=\"Country\")\n",
    "fit.columns = [\"Country\", \"Year\", \"FiT_dummy\"]\n",
    "rps = rps.melt(id_vars=\"Country\")\n",
    "rps.columns = [\"Country\", \"Year\", \"RPS_dummy\"]\n",
    "\n",
    "rep = fit.merge(rps, on=[\"Country\", \"Year\"])\n",
    "\n",
    "# policy indicators dataframe\n",
    "policies = cp.merge(rep, on=[\"Country\", \"Year\"], how=\"inner\")\n",
    "policies[[\"FiT_dummy\", \"RPS_dummy\"]] = policies[[\"FiT_dummy\", \"RPS_dummy\"]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these loops creates the indicator values for carbon taxes, emissions trading FiT, RPS based on first year of adoption \n",
    "# (as currently written, it assumes that, once implemented, policies are not repealed)\n",
    "for policy in [\"Tax_dummy\", \"ETS_dummy\", \"FiT_dummy\", \"RPS_dummy\"]:\n",
    "    for ctry in policies.Country.unique():\n",
    "        value = 0\n",
    "\n",
    "        for yr in policies.Year.unique():\n",
    "            if policies.loc[(policies.Country==ctry) & (policies.Year==yr), policy].item() == 1:\n",
    "                value = 1\n",
    "            policies.loc[(policies.Country==ctry) & (policies.Year==yr), policy] = value\n",
    "\n",
    "# policy aggregate indicators\n",
    "policies.loc[:, \"Pricing_dummy\"] = policies.loc[:, \"Tax_dummy\"] + policies.loc[:, \"ETS_dummy\"] \n",
    "policies.loc[:, \"Tech_dummy\"] = policies.loc[:, \"FiT_dummy\"] + policies.loc[:, \"RPS_dummy\"] \n",
    "\n",
    "policies.loc[policies.Pricing_dummy > 0, \"Pricing_dummy\"] = 1\n",
    "policies.loc[policies.Tech_dummy > 0, \"Tech_dummy\"] = 1\n",
    "\n",
    "policies.columns = [\"Country\", \"Year\", \"tax\", \"ets\", \"fit\", \"rps\", \"pricing\", \"techpol\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Technology proxies\n",
    "\n",
    "Proxies: \n",
    "- patents granted at the EPO for climate change mitigation technologies in electricity generation, transmission and distribution\n",
    "- installed wind and solar generation capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change names in OECD PAT file\n",
    "patents = pd.read_csv(path_root+'/OECD/Patents/PATS_IPC_09102020150032306.csv')\n",
    "patents.rename(columns={\"Value\":\"Patent_count\"}, inplace=True)\n",
    "\n",
    "recap = pd.read_csv(path_root+\"/RE capacity/RE_all.csv\")\n",
    "recap.columns = [\"Country\", \"Year\", \"capacity_ws\"]\n",
    "\n",
    "elecprod = pd.read_csv(path_root+'/IEA/IEA_Elec_prod.csv', usecols = [\"Country\", \"Product\", \"Time\", \"Value\"])\n",
    "elecprod = elecprod.groupby(by=[\"Country\", \"Time\"]).sum()\n",
    "elecprod.reset_index(inplace=True)\n",
    "\n",
    "oecd_wb_map = {\"China (People's Republic of)\": 'China', 'Egypt': 'Egypt, Arab Rep.', 'Iran': 'Iran, Islamic Rep.',\n",
    "               'Russia': 'Russian Federation', 'Venezuela': 'Venezuela, RB'}\n",
    "\n",
    "patents['Country'] = patents['Country'].replace(to_replace=oecd_wb_map)\n",
    "\n",
    "#select only \"inventors\" (following Lovely & Popp) and the patent family \n",
    "patents = patents.loc[(patents.KINDCOUNTRY==\"INVENTORS\") & (patents.KINDPATENT==\"EPO_A\") & (patents.KINDDATE==\"PRIORITY\"), :]\n",
    "\n",
    "patents.drop(['KINDCOUNTRY', 'Reference country', 'KINDPATENT', 'Patents Office & Patents Families', 'LOCATION', 'Technology domains & IPC', \n",
    "              'KINDDATE', 'PowerCode Code', 'PowerCode', 'IPC', 'TIME', 'Unit Code', 'Unit',\n",
    "              'Reference Date', 'Reference Period Code', 'Reference Period', 'Flag Codes', 'Flags'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.1 Net installed capacity stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctries = [x for x in sorted(recap.Country.unique()) if x not in [\"St. Pierre-Miquelon\", \"EU 28\"]]\n",
    "yrs = sorted(recap.Year.unique())\n",
    "\n",
    "# generating a dataframe with all years for all countries\n",
    "df = pd.DataFrame(columns=[\"Country\", \"Year\"])\n",
    "\n",
    "for ctry in ctries:\n",
    "    for yr in yrs:\n",
    "        df2 = pd.DataFrame([[ctry, yr]], columns=[\"Country\", \"Year\"])\n",
    "        df = df.append(df2, ignore_index=True)\n",
    "\n",
    "df = df.merge(recap, on=[\"Country\", \"Year\"], how=\"left\")\n",
    "df.Year = df.Year.astype(int)\n",
    "\n",
    "# Imputing missing net installed capacity data\n",
    "\n",
    "# filling nan entries of column 'capacity_ws' with zeros if the first year for which we have data is strictly lower than 15\n",
    "# This makes two assumptions:\n",
    "#    1. that no installed capacity was added AND then retired prior to first year for which data is available\n",
    "#    2. that the capacity was installed in that first year for which data was available\n",
    "# unlikely to introduce many inaccuracies in the series as \n",
    "#    1. it starts in 1990 and data is available from 2000 onward at the latest\n",
    "#    2. most countries had a an installed capacity of '0' in the first years for which data is available\n",
    "# The countries for which it might be problematic are: 'Costa Rica' (1995, 17), 'Germany' (1991, 112), 'Brazil' (2000, 22), \n",
    "# 'China' (2000, 375), 'Iran, Islamic Rep.' (2000, 11), 'Morocco' (2000, 61)\n",
    "# For these countries, net capacity data does not go back to 1990 and capacity in the first year for which data is available exceeds 10 Mw\n",
    "# a = recap.drop_duplicates(subset=[\"Country\"], keep='first')\n",
    "# a.loc[(a.capacity_ws != 0) & (a.Year > 1990) & (a.capacity_ws > 10), :]\n",
    "\n",
    "for ctry in ctries:\n",
    "    for i in range(0, 30):\n",
    "        j = 2019 - i\n",
    "        \n",
    "        is_loc = (df.Country==ctry) & (df.Year==j)\n",
    "        \n",
    "        if pd.isna(df.loc[is_loc, \"capacity_ws\"].item()) == True:\n",
    "            if df.loc[(df.Country==ctry) & (df.Year==j+1), \"capacity_ws\"].item() < 15:\n",
    "                df.loc[is_loc, \"capacity_ws\"] = 0\n",
    "    \n",
    "# CALCULATING NET CAPACITY ADDITIONS (capacity additions - capacity retirement)           \n",
    "\n",
    "for ctry in ctries:\n",
    "    \n",
    "    #Assume capacity was added in 1990 if capacity_ws in 1990 < 15 Mw\n",
    "    if df.loc[(df.Country==ctry) & (df.Year==1990), \"capacity_ws\"].item() < 15:\n",
    "        df.loc[(df.Country==ctry) & (df.Year==1990), \"capacity_ws_add\"] = df.loc[(df.Country==ctry) & (df.Year==1990), \"capacity_ws\"]\n",
    "    \n",
    "    for i in range(1991, 2020):\n",
    "        is_loc = (df.Country==ctry) & (df.Year==i)\n",
    "        df.loc[is_loc, \"capacity_ws_add\"] = df.loc[is_loc, \"capacity_ws\"].item() - df.loc[(df.Country==ctry) & (df.Year==i-1), \"capacity_ws\"].item()\n",
    "\n",
    "#if gross capacity at end of year ('capacity_ws') is naught, then we know that no capacity was added in that year and hence 'capacity_ws_add' is 0\n",
    "df.loc[df.capacity_ws==0, \"capacity_ws_add\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NET CAPACITY STOCK\n",
    "\n",
    "disc_rate = 0.04\n",
    "\n",
    "recap_cum = pd.DataFrame()\n",
    "\n",
    "for ctry in df.Country.unique():\n",
    "\n",
    "    temp_cap = df.loc[df.Country==ctry, :].copy()\n",
    "    temp_cap.reset_index(inplace=True)\n",
    "    temp_cap.drop(\"index\", axis=1, inplace=True)\n",
    "    temp_cap[\"discount_stock\"] = np.nan\n",
    "    \n",
    "    # Following from assumption above, if capacity_ws in 1990 < 15 Mw, it is assumed that it was added in 1990;\n",
    "    # Hence the discounted sctock is equal to capacity_ws_add\n",
    "    if temp_cap.loc[0, \"capacity_ws_add\"] < 15:\n",
    "        temp_cap.loc[0, \"discount_stock\"] = temp_cap.loc[0, \"capacity_ws_add\"]\n",
    "        \n",
    "        for i in range(1, len(temp_cap)):\n",
    "            temp_cap.loc[i, \"discount_stock\"] = temp_cap.loc[i-1, \"discount_stock\"]/(1+disc_rate) + temp_cap.loc[i, \"capacity_ws_add\"]\n",
    "\n",
    "    # for this group of countries, we know the first year of renewable production, thanks to IEA production data\n",
    "    # we use that year together with actual production data to estimate initial stock \n",
    "    elif ctry in elecprod.Country.unique():\n",
    "        \n",
    "        temp_prod = elecprod.loc[elecprod.Country==ctry,:].copy()\n",
    "        temp_prod.reset_index(inplace=True)\n",
    "        temp_prod.drop(\"index\", axis=1, inplace=True)\n",
    "        temp_prod[\"gr_rate\"] = np.nan\n",
    "        \n",
    "        first_addition_index = temp_cap.capacity_ws_add.first_valid_index()\n",
    "        first_addition_year = temp_cap.loc[first_addition_index, \"Year\"]\n",
    "\n",
    "        first_production_index = temp_prod.Value.loc[temp_prod.Value!=0].index[0].item()\n",
    "        first_production_year = temp_prod.loc[first_production_index, \"Time\"]\n",
    "        \n",
    "        for i in range(first_production_index+1, len(temp_prod)):\n",
    "            temp_prod.loc[i, \"gr_rate\"] = (temp_prod.loc[i, \"Value\"]-temp_prod.loc[i-1, \"Value\"])/temp_prod.loc[i-1, \"Value\"]\n",
    "        \n",
    "        # calculate the ratio in second year to make sure capacity is operational and generation value > 0 \n",
    "        ratio = temp_prod.loc[temp_prod.Time==first_addition_year+1, \"Value\"].item()/temp_cap.loc[temp_cap.Year == first_addition_year+1, \"capacity_ws\"].item()\n",
    "        \n",
    "        cap_first_year = temp_prod.loc[temp_prod.Time==first_production_year, \"Value\"].item()/ratio\n",
    "        \n",
    "        temp_prod.loc[temp_prod.Time==first_production_year, \"capacity_ws\"] = cap_first_year\n",
    "        \n",
    "        for yr in range(first_production_year+1, first_addition_year+1):\n",
    "            temp_prod.loc[temp_prod.Time==yr, \"capacity_ws\"] = temp_prod.loc[temp_prod.Time==yr-1, \"capacity_ws\"].item()*(1+temp_prod.loc[temp_prod.Time==yr, \"gr_rate\"].item())\n",
    "        \n",
    "        temp_prod[\"capacity_ws_add\"] = np.nan\n",
    "        temp_prod.loc[temp_prod.Time==first_production_year, \"capacity_ws_add\"] = temp_prod.loc[temp_prod.Time==first_production_year, \"capacity_ws\"]\n",
    "        \n",
    "        for yr in range(first_production_year+1, first_addition_year+1):\n",
    "            temp_prod.loc[temp_prod.Time==yr, \"capacity_ws_add\"] = temp_prod.loc[temp_prod.Time==yr, \"capacity_ws\"].item() - temp_prod.loc[temp_prod.Time==yr-1, \"capacity_ws\"].item()\n",
    "            \n",
    "        temp_prod[\"capacity_ws_ds\"] = np.nan\n",
    "        temp_prod.loc[temp_prod.Time==first_production_year, \"capacity_ws_ds\"] = temp_prod.loc[temp_prod.Time==first_production_year, \"capacity_ws\"]\n",
    "        \n",
    "        # NET STOCK\n",
    "        for yr in range(first_production_year+1, first_addition_year+1):\n",
    "            temp_prod.loc[temp_prod.Time==yr, \"capacity_ws_ds\"] = temp_prod.loc[temp_prod.Time==yr-1, \"capacity_ws_ds\"].item()/(1+disc_rate) + temp_prod.loc[temp_prod.Time==yr, \"capacity_ws_add\"].item()\n",
    "                \n",
    "        temp_cap.loc[temp_cap.Year==first_addition_year-1, \"discount_stock\"] = temp_prod.loc[temp_prod.Time==first_addition_year-1, \"capacity_ws_ds\"].item()\n",
    "        \n",
    "        first_ds_index = temp_cap.discount_stock.first_valid_index()\n",
    "        \n",
    "        for i in range(first_ds_index+1, len(temp_cap)):\n",
    "            temp_cap.loc[i, \"discount_stock\"] = temp_cap.loc[i-1, \"discount_stock\"]/(1+disc_rate) + temp_cap.loc[i, \"capacity_ws_add\"]\n",
    "    \n",
    "#    elif ctry in ['Brazil', 'China', 'Costa Rica', 'India', 'Morocco']:\n",
    "    \n",
    "    recap_cum = pd.concat([recap_cum, temp_cap])\n",
    "\n",
    "recap_cum.drop([\"capacity_ws_add\", \"capacity_ws\"], axis=1, inplace=True)\n",
    "recap_cum.columns = [\"Country\", \"Year\", \"capacity_ws_ds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2 Discounted cumulative patents stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patents_cum = pd.DataFrame()\n",
    "\n",
    "for ctry in patents.Country.unique():\n",
    "    temp = patents.loc[patents.Country==ctry, :].copy()\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp.drop(\"index\", axis=1, inplace=True)\n",
    "    temp[\"discount_stock\"] = np.nan\n",
    "    \n",
    "    # initializing discounted stock at value of first patent addition\n",
    "    temp.loc[0, \"discount_stock\"] = temp.loc[0, \"Patent_count\"]\n",
    "    \n",
    "    for i in range(1, len(temp)):\n",
    "        temp.loc[i, \"discount_stock\"] = temp.loc[i-1, \"discount_stock\"]/1.1 + temp.loc[i, \"Patent_count\"]\n",
    "    \n",
    "    patents_cum = pd.concat([patents_cum, temp])\n",
    "\n",
    "patents_cum.drop(\"Patent_count\", axis=1, inplace=True)\n",
    "patents_cum.columns = [\"Country\", \"Year\", \"patents_ds\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Bilateral (dyadic) weights\n",
    "## II.1 International trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path_root+'/Bilateral trade/IMF_DoT_Stats') #sets the current directory to 'indir'\n",
    "fileList=glob.glob(\"*.csv\") #this command generates a list of csv files\n",
    "# to concatenate, we will stack the files into a single Python list\n",
    "# the method will generate a single output file as output\n",
    "# before starting the loop, we need to create an empty list object\n",
    "\n",
    "dfList = []\n",
    "\n",
    "colnames = [\"REPORTER_COUNTRY\",\"Reporter Country\",\"FLOW\",\"Flow\",\"LOCATION\",\"Partner Country\",\"FREQUENCY\",\"Frequency\",\"TIME\",\"Time\",\"Value\",\"Flag Codes\",\"Flags\"]\n",
    "\n",
    "#each iteration of the loop will add a dataframe to the list\n",
    "for filename in fileList:\n",
    "    #print(filename)\n",
    "    df=pd.read_csv(filename, header=0)\n",
    "    dfList.append(df)\n",
    "concatDf=pd.concat(dfList,axis=0) #'axis=0' means that we are concatenating vertically, if we wanted to concatenate horizontally, we should use 'axis=1'\n",
    "concatDf.columns=colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilat_trade = concatDf.rename(columns={'Reporter Country': 'Reporter_Country', 'Partner Country': 'Partner_Country'})\n",
    "\n",
    "imf_wb_map = {'Afghanistan, Islamic Republic of': 'Afghanistan', 'Armenia, Republic of': 'Armenia', 'Azerbaijan, Republic of': 'Azerbaijan',\n",
    "              'Bahrain, Kingdom of': 'Bahrain', 'China, P.R.: Mainland': 'China', \"CÃ\\x83Â´te d'Ivoire\": \"Cote d'Ivoire\", 'Curaçao':\"Curacao\",\n",
    "              \"CÃ´te d'Ivoire\": \"Cote d'Ivoire\", \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'The Bahamas': 'Bahamas, The', 'Brunei': 'Brunei Darussalam',\n",
    "              'Congo, Democratic Republic of': 'Congo, Dem. Rep.', 'Congo, Republic of': 'Congo, Rep.', 'Egypt': 'Egypt, Arab Rep.',\n",
    "              'The Gambia': 'Gambia, The', 'China, P.R.: Hong Kong': 'Hong Kong SAR, China', 'Iran, Islamic Republic of': 'Iran, Islamic Rep.',\n",
    "              'Korea, Republic of': 'Korea, Rep.', \"Korea, Democratic People's Republic of\": 'Korea, Dem. Rep.',\n",
    "              'Kosovo, Republic of': 'Kosovo', \"Lao People's Democratic Republic\": 'Lao PDR', \n",
    "              'Marshall Islands, Republic of': 'Marshall Islands', 'China, P.R.: Macao': 'Macao SAR, China', \n",
    "              \"North Macedonia, Republic of\":\"North Macedonia\", 'Russia': 'Russian Federation', 'São Tomé & Príncipe': 'Sao Tome and Principe',\n",
    "              'Serbia, Republic of': 'Serbia', 'Timor-Leste, Democratic Republic of':'Timor-Leste',\n",
    "              'Venezuela, Republica Bolivariana de': 'Venezuela, RB', 'Yemen, Republic of': 'Yemen, Rep.'}\n",
    "\n",
    "# Replace names\n",
    "bilat_trade['Reporter_Country'] = bilat_trade['Reporter_Country'].replace(to_replace=imf_wb_map)\n",
    "bilat_trade['Partner_Country'] = bilat_trade['Partner_Country'].replace(to_replace=imf_wb_map)\n",
    "\n",
    "# Drop non-country 'Reporter' and 'Partner'\n",
    "excl_reporters = [\"Sub-Saharan Africa\"]\n",
    "\n",
    "excl_partners = ['Advanced Economies','Africa','Africa not specified','Asia not specified','Countries&Areas, not specified','CIS',\n",
    "            'Developing Asia','Emerging and Developing Asia','Emerging and Developing Economies','Emerging and Developing Europe',\n",
    "            'Euro Area','Europe','European Union','Europe not specified','Export Earnings: Fuel','Export Earnings: Non-Fuel',\n",
    "            'Middle East','Middle East not specified','Middle East, North Africa & Pakistan','Other Countries n.i.e.',\n",
    "            'Sub-Saharan Africa (developing only)','Sub-Saharan Africa','Western Hemisphere','Western Hem. not specified','SACCA']\n",
    "\n",
    "bilat_trade = bilat_trade.loc[(~bilat_trade.Partner_Country.isin(excl_partners)) & (~bilat_trade.Reporter_Country.isin(excl_reporters)), :] \n",
    "bilat_trade.drop([\"REPORTER_COUNTRY\", \"LOCATION\", \"FREQUENCY\", \"Frequency\", \"Flow\", \"TIME\", \"Flag Codes\", \"Flags\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeflows = {}\n",
    "tradeflows_share = {}\n",
    "tradeflows_wld = {}\n",
    "\n",
    "flow_names = {\"TMG_CIF_USD\":\"IMP\", \"TXG_FOB_USD\":\"EXP\", \"TMG_CIF_USD+TMG_FOB_USD\":\"IMPEXP\"}\n",
    "\n",
    "imp_exp_bilat = bilat_trade.groupby([\"Reporter_Country\", \"Partner_Country\", \"Time\"]).sum()\n",
    "imp_exp_bilat.reset_index(inplace=True)\n",
    "imp_exp_bilat[\"FLOW\"] = \"TMG_CIF_USD+TMG_FOB_USD\"\n",
    "\n",
    "bilat_trade = pd.concat([bilat_trade, imp_exp_bilat])\n",
    "\n",
    "for flow in [\"TMG_CIF_USD\", \"TXG_FOB_USD\", \"TMG_CIF_USD+TMG_FOB_USD\"]:\n",
    "    tradeflows[flow_names[flow]] = bilat_trade[bilat_trade[\"FLOW\"]==flow]\n",
    "    tradeflows_wld[flow_names[flow]] = tradeflows[flow_names[flow]][tradeflows[flow_names[flow]][\"Partner_Country\"]==\"World\"]\n",
    "    \n",
    "    tradeflows_share[flow_names[flow]] = tradeflows[flow_names[flow]].merge(tradeflows_wld[flow_names[flow]], on=[\"Reporter_Country\", \"FLOW\", \"Time\"])\n",
    "    tradeflows_share[flow_names[flow]].rename(columns={\"Value_x\":\"Value\", \"Value_y\":\"Total_year\", \"Partner_Country_x\":\"Partner_Country\"}, inplace=True)\n",
    "    tradeflows_share[flow_names[flow]].drop([\"Partner_Country_y\"], axis=1, inplace=True)\n",
    "\n",
    "    tradeflows_share[flow_names[flow]][\"Value_share_tot\"] = tradeflows_share[flow_names[flow]].Value/tradeflows_share[flow_names[flow]].Total_year\n",
    "    tradeflows_share[flow_names[flow]] = tradeflows_share[flow_names[flow]].loc[tradeflows_share[flow_names[flow]].Partner_Country != \"World\", :] #removing 'World' entry from partners' list\n",
    "    tradeflows_share[flow_names[flow]].drop([\"Value\", \"Total_year\"], axis=1, inplace=True)\n",
    "    tradeflows_share[flow_names[flow]].columns = [\"Reporter\", \"FLOW\", \"Partner\", \"Year\", \"Trade_flow_share\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25 'Partner' entities in the bilateral trade dataset are not present in the `policies` dataset.\n",
    "# Some of them ('U.S.S.R.', 'Yugoslavia, SFR', 'Czechoslovakia', 'Serbia and Montenegro') are entities that have ceased to exist and \n",
    "# are now split into multiple countries. The `policies` dataset records entries for their successor entities.\n",
    "# Remaining entities are simply absent from the `policies` dataset. However, this means that none of the policies were introduced in any of these entities. Hence\n",
    "# we interpret absence from dataset as '0'.\n",
    "\n",
    "# However none of these 'Partner' entities had a \n",
    "policy_trade_partners = set(tradeflows_share[\"IMP\"].Partner.unique())-set(policies.Country.unique()).intersection(set(tradeflows_share[\"IMP\"].Partner.unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Shared institutional and cultural ties\n",
    "### I.2.1. Membership of multilateral organisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EU\n",
    "\n",
    "#0. List of all national jurisdictions - WB names, taken from PE name match file\n",
    "country_list = [\"Afghanistan\",\"Albania\",\"Algeria\",\"Andorra\",\"Angola\",\"Antigua and Barbuda\",\"Argentina\",\"Armenia\",\n",
    "                \"Australia\",\"Austria\",\"Azerbaijan\",\"Bahamas, The\",\"Bahrain\",\"Bangladesh\",\"Barbados\",\"Belarus\",\n",
    "                \"Belgium\",\"Belize\",\"Benin\",\"Bhutan\",\"Bolivia\",\"Bosnia and Herzegovina\",\"Botswana\",\"Brazil\",\n",
    "                \"Brunei Darussalam\",\"Bulgaria\",\"Burkina Faso\",\"Burundi\",\"Cabo Verde\",\"Cambodia\",\"Cameroon\",\n",
    "                \"Canada\",\"Central African Republic\",\"Chad\",\"Chile\",\"China\",\"Colombia\",\"Comoros\",                \n",
    "                \"Congo, Dem. Rep.\",\"Congo, Rep.\",\"Costa Rica\",\"Cote d'Ivoire\",\"Croatia\",\"Cuba\",\"Cyprus\",\n",
    "                \"Czech Republic\",\"Denmark\",\"Djibouti\",\"Dominica\",\"Dominican Republic\",\"Ecuador\",\"Egypt, Arab Rep.\",\n",
    "                \"El Salvador\",\"Equatorial Guinea\",\"Eritrea\",\"Estonia\",\"Ethiopia\",\"Fiji\",\"Finland\",\"France\",\"Gabon\",\n",
    "                \"Gambia, The\",\"Georgia\",\"Germany\",\"Ghana\",\"Greece\",\"Grenada\",\"Guatemala\",\"Guinea\",\"Guinea-Bissau\",\n",
    "                \"Guyana\",\"Haiti\",\"Honduras\",\"Hong Kong SAR, China\",\"Hungary\",\"Iceland\",\n",
    "                \"India\",\"Indonesia\",\"Iran, Islamic Rep.\",\"Iraq\",\"Ireland\",\"Israel\",\"Italy\",\"Jamaica\",\"Japan\",\n",
    "                \"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kiribati\",\"Korea, Dem. Rep.\",\"Korea, Rep.\",\"Kosovo\",\"Kuwait\",\n",
    "                \"Kyrgyz Republic\",\"Lao PDR\",\"Latvia\",\"Lebanon\",\"Lesotho\",\"Liberia\",\"Libya\",\"Liechtenstein\",\n",
    "                \"Lithuania\",\"Luxembourg\",\"Macao SAR, China\",\"North Macedonia\", \"Madagascar\",\"Malawi\",\n",
    "                \"Malaysia\",\"Maldives\",\"Mali\",\"Malta\",\"Marshall Islands\",\"Mauritania\",\"Mauritius\",\"Mexico\",\n",
    "                \"Moldova\",\"Monaco\",\"Mongolia\",\"Montenegro\",\"Morocco\",\"Mozambique\",\"Myanmar\",\"Namibia\",\"Nepal\",\n",
    "                \"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Niger\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Palau\",\n",
    "                \"Panama\",\"Papua New Guinea\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Puerto Rico\",\n",
    "                \"Qatar\",\"Romania\",\"Russian Federation\",\"Rwanda\",\"Samoa\",\"San Marino\",\"Sao Tome and Principe\",\n",
    "                \"Saudi Arabia\",\"Senegal\",\"Serbia\",\"Seychelles\",\"Sierra Leone\",\"Singapore\",\"Slovak Republic\",\n",
    "                \"Slovenia\",\"Solomon Islands\",\"Somalia\",\"South Africa\",\"South Sudan\",\"Spain\",\"Sri Lanka\",\n",
    "                \"St. Kitts and Nevis\",\"St. Lucia\",\"St. Vincent and the Grenadines\",\"Sudan\",\"Suriname\",\"Swaziland\",\n",
    "                \"Sweden\",\"Switzerland\",\"Syrian Arab Republic\",\"Tajikistan\",\"Tanzania\",\"Thailand\",\"Timor-Leste\",\n",
    "                \"Togo\",\"Tonga\",\"Trinidad and Tobago\",\"Tunisia\",\"Turkey\",\"Turkmenistan\",\"Tuvalu\",\"Uganda\",\n",
    "                \"Ukraine\",\"United Arab Emirates\",\"United Kingdom\",\"United States\",\"Uruguay\",\"Uzbekistan\",\"Vanuatu\",\n",
    "                \"Venezuela, RB\",\"Vietnam\",\"West Bank and Gaza\",\"Yemen, Rep.\",\"Zambia\",\"Zimbabwe\"]\n",
    "\n",
    "years = [str(i) for i in range(1990,2019)]\n",
    "\n",
    "#1. List of EU member states in each year\n",
    "eu_list = ['Belgium','France','Netherlands','Germany','Italy','Luxembourg',\n",
    "          'Denmark','Ireland','United Kingdom','Greece','Spain','Portugal',\n",
    "          'Austria','Sweden','Finland','Czech Republic','Slovenia','Poland',\n",
    "          'Slovakia','Estonia','Cyprus','Latvia','Lithuania','Malta','Hungary',\n",
    "          'Romania','Bulgaria']\n",
    "\n",
    "eu_dic = {'Belgium':1957,'France':1957,'Netherlands':1957,'Germany':1957,'Italy':1957,'Luxembourg':1957,\n",
    "          'Denmark':1973,'Ireland':1973,'United Kingdom':1973,'Greece':1981,'Spain':1986,'Portugal':1986,\n",
    "          'Austria':1995,'Sweden':1995,'Finland':1995,'Czech Republic':2004,'Slovenia':2004,'Poland':2004,\n",
    "          'Slovakia':2004,'Estonia':2004,'Cyprus':2004,'Latvia':2004,'Lithuania':2004,'Malta':2004,'Hungary':2004,\n",
    "          'Romania':2007,'Bulgaria':2007}\n",
    "\n",
    "eu_dic_II = {}\n",
    "\n",
    "for country in eu_list:\n",
    "    for year in years:\n",
    "        if int(year) >= eu_dic[country]:\n",
    "            if year not in eu_dic_II:\n",
    "                eu_dic_II[year] = [] \n",
    "            eu_dic_II[year].append(country)\n",
    "\n",
    "\n",
    "eu_mat = {}\n",
    "\n",
    "\n",
    "for reporter in country_list:\n",
    "    if reporter not in eu_mat:\n",
    "        eu_mat[reporter] = {}\n",
    "    for partner in country_list:\n",
    "        if partner not in eu_mat[reporter]:\n",
    "            eu_mat[reporter][partner] = {}\n",
    "        if partner != reporter:\n",
    "            for year in years:\n",
    "                if year not in eu_mat[reporter][partner]:\n",
    "                    if reporter in eu_dic_II[year] and partner in eu_dic_II[year]:\n",
    "                        eu_mat[reporter][partner][year] = 1\n",
    "                    else:\n",
    "                        eu_mat[reporter][partner][year] = 0\n",
    "\n",
    "with open(path_root_ii+'/data/raw_data/constructed_data/EU_bilat.csv', \"w\") as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(('Reporter','Partner','Year','EU'))\n",
    "\n",
    "    for reporter in eu_mat:\n",
    "        for partner in eu_mat[reporter]:\n",
    "            for year in eu_mat[reporter][partner]:\n",
    "                writer.writerow((reporter,partner,year,eu_mat[reporter][partner][year]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_matrix = pd.read_csv(path_root_ii+'/data/raw_data/constructed_data/EU_bilat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.2. CEPII\n",
    "The CEPII data is already in the form of a dyadic matrix; we only need to decide which variables we want too keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cepii_wb_map = {'Brunei': 'Brunei Darussalam', 'Cape Verde': 'Cabo Verde', \"China, People's Republic of\": 'China',\n",
    "                'Egypt': 'Egypt, Arab Rep.', 'Hong Kong': 'Hong Kong SAR, China', 'Iran': 'Iran, Islamic Rep.', 'Kyrgyzstan': 'Kyrgyz Republic',\n",
    "                'Laos': 'Lao PDR', 'Macau': 'Macao SAR, China', 'Russia': 'Russian Federation', 'Saint Kitts and Nevis': 'St. Kitts and Nevis',\n",
    "                'Saint Lucia': 'St. Lucia', 'Saint Vincent and the Grenadines': 'St. Vincent and the Grenadines', 'Slovakia': 'Slovak Republic',\n",
    "                'Syria': 'Syrian Arab Republic', 'Venezuela': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtype option: we need to specify some (data) types of the columns because some of them contain multiple types (i.e. string and NaN)\n",
    "#not specifying the data type will prompt Python to issue a warning but should not stop the code from running\n",
    "cepii_matrix = pd.read_csv(path_root+'/Gravity models/CEPII-Excel files/gravdata_cepii.csv', dtype={\"iso2_o\":str,\"empire\":str,\"legold_o\":str,\"legnew_o\":str})\n",
    "\n",
    "#dropping year before 1990\n",
    "cepii_matrix = cepii_matrix.loc[cepii_matrix['year']>=1990]\n",
    "#keep only selected columns\n",
    "cepii_matrix = cepii_matrix[[\"iso3_o\", \"iso3_d\", \"year\", \"contig\", \"comlang_off\", \"comrelig\", \"comcol\", \"col45\", \n",
    "                             \"fta_bb\", \"fta_hmr\", \"fta_wto\"]]\n",
    "\n",
    "#replace country codes in CEPII dataframe to reflect latest country code\n",
    "cepii_matrix.replace(to_replace=[\"ROM\", \"ZAR\", \"TMP\"], value=[\"ROU\", \"COD\", \"TLS\"], inplace=True)\n",
    "\n",
    "ctry_codes = pd.read_csv(path_root+'/Gravity models/ISO-country_codes_II.csv')\n",
    "\n",
    "#selection of the two columns in csv_data_II\n",
    "ctry_codes = ctry_codes[['Common Name','ISO 3166-1 3 Letter Code']]\n",
    "\n",
    "#by specifying left I am using keys of left data frame only\n",
    "cepii_matrix = pd.merge(cepii_matrix,ctry_codes, how='left', left_on=['iso3_o'], right_on=['ISO 3166-1 3 Letter Code'])\n",
    "cepii_matrix = pd.merge(cepii_matrix,ctry_codes, how='left', left_on=['iso3_d'], right_on=['ISO 3166-1 3 Letter Code'])\n",
    "\n",
    "cepii_matrix.drop([\"ISO 3166-1 3 Letter Code_x\", \"ISO 3166-1 3 Letter Code_y\"], axis=1, inplace=True)\n",
    "cepii_matrix.rename(columns={\"Common Name_x\":\"Common_name_o\", \"Common Name_y\":\"Common_name_d\"}, inplace=True)\n",
    "cepii_matrix = cepii_matrix[[\"iso3_o\", \"Common_name_o\", \"iso3_d\", \"Common_name_d\", \"year\", \"contig\", 'comlang_off', 'comrelig', 'comcol',\n",
    "                             \"col45\", 'fta_hmr', 'fta_wto']] #'fta_bb', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:1765: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(loc, value)\n"
     ]
    }
   ],
   "source": [
    "#drop iso3_o x iso3_d entries\n",
    "cepii_matrix = cepii_matrix.loc[(cepii_matrix.iso3_o != cepii_matrix.iso3_d), :]\n",
    "cepii_matrix = cepii_matrix.drop([\"iso3_o\", \"iso3_d\"], axis=1)\n",
    "cepii_matrix.columns = [\"Reporter\", \"Partner\", \"Year\", \"contig\", \"comlang_off\", \"comrelig\", \"comcol\", \"col45\", \"fta_hmr\", \"fta_wto\"]\n",
    "\n",
    "cepii_matrix[\"Reporter\"] = cepii_matrix[\"Reporter\"].replace(to_replace=cepii_wb_map)\n",
    "cepii_matrix[\"Partner\"] = cepii_matrix[\"Partner\"].replace(to_replace=cepii_wb_map)\n",
    "\n",
    "# Extension of cepii matrix through 2018\n",
    "cepii_matrix_2015 = cepii_matrix.loc[(cepii_matrix.Year == 2015), :]\n",
    "\n",
    "for y in [2016, 2017, 2018]:\n",
    "    temp = cepii_matrix_2015\n",
    "    temp.loc[:, \"Year\"] = y\n",
    "    \n",
    "    cepii_matrix = pd.concat([cepii_matrix, temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2.3. Bilateral assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "odaI = pd.read_csv(path_root+'/OECD/ODA/TABLE2A_05102020122645440.csv')\n",
    "odaII = pd.read_csv(path_root+'/OECD/ODA/TABLE2A_05102020122754846.csv')\n",
    "oda = pd.concat([odaI,odaII])\n",
    "\n",
    "oda = oda[oda['Aid type']=='Memo: ODA Total, Gross disbursements']\n",
    "oda = oda[oda[\"Amount type\"]==\"Constant Prices\"]\n",
    "\n",
    "oda.drop_duplicates(inplace=True)\n",
    "\n",
    "oda_tot = oda.groupby(by=[\"Recipient\", \"Year\"]).sum()\n",
    "oda_tot.drop([\"DONOR\", \"RECIPIENT\", \"AIDTYPE\", \"PART\", \"TIME\", \"Flag Codes\", \"Flags\", 'Reference Period Code', 'PowerCode Code', 'Reference Period'], \n",
    "             axis=1, inplace=True)\n",
    "oda_tot.reset_index(inplace=True)\n",
    "oda_tot.rename(columns={\"Value\":\"ODA_tot\"}, inplace=True)\n",
    "\n",
    "oda = oda.merge(oda_tot, on=[\"Recipient\", \"Year\"])\n",
    "\n",
    "oda[\"ODA_share\"] = oda.Value/oda.ODA_tot\n",
    "\n",
    "oda_matrix = oda[[\"Recipient\", \"Donor\", \"Year\", \"ODA_share\"]]\n",
    "oda_matrix.columns = [\"Reporter\", \"Partner\", \"Year\", \"ODA_share\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.3 Removal of 'self-distances' (and matrix normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all dyadic matrices\n",
    "dyad_mat = [tradeflows_share[\"IMP\"], tradeflows_share[\"EXP\"], tradeflows_share[\"IMPEXP\"], eu_matrix, cepii_matrix, oda_matrix]\n",
    "\n",
    "for mat in dyad_mat:\n",
    "    mat = mat.loc[mat.Reporter!=mat.Partner, :]\n",
    "    \n",
    "#Normalization - year by year! (see Lee & Yu, 2012)\n",
    "#i = 0\n",
    "#for mat in [eu_matrix]:#dyad_mat:\n",
    "#    for yr in mat.Year.unique():\n",
    "#        temp = mat.loc[mat.Year==yr, :]\n",
    "#        temp.drop(\"Year\", axis=1, inplace=True)\n",
    "        \n",
    "#        for col in temp.columns[2:]:\n",
    "            #transform into matrix format\n",
    "#            tempII = temp[[\"Reporter\", \"Partner\", col]]\n",
    "#            tempII = tempII.pivot(index='Reporter', columns='Partner')[col]\n",
    "#            tempII = tempII.fillna(0)\n",
    "#            tempII = tempII.to_numpy()\n",
    "\n",
    "            #calculate eigenvalues; extract largest\n",
    "#            w, v = LA.eig(tempII)\n",
    "#            max_w = max(w)\n",
    "            \n",
    "            #divide values in orginal dataframe by that largest eigenvalue\n",
    "#            mat.loc[mat.Year==yr, col] = mat.loc[mat.Year==yr, col]/max_w\n",
    "#    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Covariates\n",
    "\n",
    "## III.1 Policy information/experience diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusion channels: EU, trade, cultural similarities, official development aid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_indicators = [\"tax\", \"ets\", \"fit\", \"rps\", \"pricing\", \"techpol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policy = policies[[\"Country\", \"Year\"]]\n",
    "df_policy.columns = [\"Reporter\", \"Year\"]\n",
    "\n",
    "for mat in dyad_mat:\n",
    "    dyad_mat_pol = mat.merge(policies, left_on=[\"Partner\", \"Year\"], right_on=[\"Country\", \"Year\"], how='left')\n",
    "    dyad_mat_pol.drop(\"Country\", axis=1, inplace=True)\n",
    "    \n",
    "    for var in policy_indicators:\n",
    "        weights = [x for x in list(mat.columns) if x not in [\"Reporter\", \"Partner\", \"Year\", \"FLOW\"]]\n",
    "        for col in weights:\n",
    "            dyad_mat_pol[var+\"_\"+col] = dyad_mat_pol[var]*dyad_mat_pol[col]\n",
    "            dyad_mat_pol[var+\"_\"+col] = dyad_mat_pol[var+\"_\"+col].astype(float)\n",
    "    \n",
    "    dyad_mat_pol.drop(policy_indicators, axis=1, inplace=True)\n",
    "    dyad_mat_pol.drop(weights, axis=1, inplace=True)\n",
    "    dyad_mat_pol = dyad_mat_pol.groupby(by=[\"Reporter\", \"Year\"]).sum()\n",
    "    dyad_mat_pol.reset_index(inplace=True)\n",
    "    \n",
    "    df_policy = df_policy.merge(dyad_mat_pol, on=[\"Reporter\", \"Year\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_policy.columns = ['Reporter', 'Year', \n",
    "                     'tax_imp', 'ets_imp', 'fit_imp', 'rps_imp', 'pricing_imp', 'techpol_imp',\n",
    "                     'tax_exp', 'ets_exp', 'fit_exp', 'rps_exp', 'pricing_exp', 'techpol_exp',\n",
    "                     'tax_impexp', 'ets_impexp', 'fit_impexp', 'rps_impexp', 'pricing_impexp', 'techpol_impexp',\n",
    "                     'tax_eu', 'ets_eu', 'fit_eu', 'rps_eu', 'pricing_eu', 'techpol_eu',\n",
    "                     'tax_contig', 'tax_comlang_off', 'tax_comrelig', 'tax_comcol', 'tax_col45', 'tax_fta_hmr', 'tax_fta_wto',\n",
    "                     \"ets_contig\", 'ets_comlang_off', 'ets_comrelig', 'ets_comcol', 'ets_col45', 'ets_fta_hmr', 'ets_fta_wto',\n",
    "                     \"fit_contig\", 'fit_comlang_off', 'fit_comrelig', 'fit_comcol', 'fit_col45', 'fit_fta_hmr', 'fit_fta_wto',\n",
    "                     \"rps_contig\", 'rps_comlang_off', 'rps_comrelig', 'rps_comcol', 'rps_col45', 'rps_fta_hmr', 'rps_fta_wto',\n",
    "                     \"pricing_contig\", 'pricing_comlang_off', 'pricing_comrelig', 'pricing_comcol', 'pricing_col45', 'pricing_fta_hmr', 'pricing_fta_wto',\n",
    "                     \"techpol_contig\", 'techpol_comlang_off', 'techpol_comrelig', 'techpol_comcol', 'techpol_col45', 'techpol_fta_hmr', 'techpol_fta_wto',\n",
    "                     'tax_ODA', 'ets_ODA', 'fit_ODA', 'rps_ODA', 'pricing_ODA', 'techpol_ODA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2 Learning from success\n",
    "Difference between average power sector CO2 emissions (per capita) of countries with and without policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'co2_em_elec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4d3286965f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mco2_power\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco2_em_elec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwb_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pop_tot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mco2_power_pc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco2_power\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mco2_power_pc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"co2_power_pc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mco2_power_pc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco2em\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mco2_power_pc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop_tot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'co2_em_elec' is not defined"
     ]
    }
   ],
   "source": [
    "co2_power = co2_em_elec\n",
    "population = wb_form[[\"Country\", \"Year\", \"pop_tot\"]]\n",
    "\n",
    "co2_power_pc = co2_power.merge(population, on=[\"Country\", \"Year\"])\n",
    "co2_power_pc[\"co2_power_pc\"] = co2_power_pc.co2em*1000/co2_power_pc.pop_tot\n",
    "\n",
    "success = co2_power_pc.merge(policies, on=[\"Country\", \"Year\"])\n",
    "\n",
    "for policy in [\"FiT_dummy\"]:\n",
    "    temp = success.groupby(by=[policy, \"Year\"]).mean()\n",
    "    temp.reset_index(inplace=True)\n",
    "    temp = temp[[\"FiT_dummy\", \"Year\", \"co2_power_pc\"]]\n",
    "\n",
    "    temp_0 = temp.loc[temp.FiT_dummy==0]\n",
    "    temp_1 = temp.loc[temp.FiT_dummy==1]\n",
    "    \n",
    "    temp = temp_0.merge(temp_1, on=[\"Year\"])\n",
    "    temp[\"co2_power_pc_diff\"] = temp.co2_power_pc_x - temp.co2_power_pc_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3 Technology\n",
    "\n",
    "1. Diffusion channels: EU, trade\n",
    "\n",
    "### III.3.1 Dyadic weighted knowledge and technology stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyad_tech = [tradeflows_share[\"IMP\"], tradeflows_share[\"EXP\"], eu_matrix]\n",
    "\n",
    "df_tech = policies[[\"Country\", \"Year\"]]\n",
    "df_tech.columns = [\"Reporter\", \"Year\"]\n",
    "\n",
    "for mat in dyad_tech:\n",
    "    for df in [patents_cum, recap_cum]:\n",
    "        dyad_mat_tech = mat.merge(df, left_on=[\"Partner\", \"Year\"], right_on=[\"Country\", \"Year\"])\n",
    "        dyad_mat_tech.drop(\"Country\", axis=1, inplace=True)\n",
    "        \n",
    "        for var in list(df.columns)[2:]:\n",
    "            weights = [x for x in list(mat.columns) if x not in [\"Reporter\", \"Partner\", \"Year\", \"FLOW\"]]\n",
    "            for col in weights:\n",
    "                dyad_mat_tech[var+\"_\"+col] = dyad_mat_tech[var]*dyad_mat_tech[col]\n",
    "\n",
    "        dyad_mat_tech.drop(list(df.columns)[2:], axis=1, inplace=True)\n",
    "        dyad_mat_tech.drop(weights, axis=1, inplace=True)\n",
    "        dyad_mat_tech = dyad_mat_tech.groupby(by=[\"Reporter\", \"Year\"]).sum()\n",
    "        dyad_mat_tech.reset_index(inplace=True)\n",
    "\n",
    "        df_tech = df_tech.merge(dyad_mat_tech, on=[\"Reporter\", \"Year\"], how=\"left\")\n",
    "\n",
    "#rename columns\n",
    "df_tech.rename(columns=dict(zip(['patents_ds_Trade_flow_share_x', 'capacity_ws_ds_Trade_flow_share_x', 'patents_ds_Trade_flow_share_y',\n",
    "                                  'capacity_ws_ds_Trade_flow_share_y', 'patents_ds_EU','capacity_ws_ds_EU'],\n",
    "                                 ['patents_ds_imp', 'capacity_ws_ds_imp', 'patents_ds_exp', 'capacity_ws_ds_exp', 'patents_ds_eu', \n",
    "                                  'capacity_ws_ds_eu'])), inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.2 Country-specific global knowledge and technology stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_re = recap_cum[[\"Country\", \"Year\"]]\n",
    "stock_pat = patents_cum[[\"Country\", \"Year\"]]\n",
    "\n",
    "for ctry in recap_cum.Country.unique():\n",
    "    temp = recap_cum.loc[recap_cum.Country!=ctry, :].groupby(\"Year\").sum()\n",
    "    \n",
    "    stock_re.loc[stock_re.Country==ctry, \"capacity_ds_global\"] = np.array(temp.iloc[:, 0])\n",
    "\n",
    "for ctry in patents_cum.Country.unique():\n",
    "    temp = patents_cum.loc[patents_cum.Country!=ctry, :].groupby(\"Year\").sum()\n",
    "    \n",
    "    stock_pat.loc[stock_pat.Country==ctry, \"patents_ds_global\"] = np.array(temp.iloc[:, 0])\n",
    "\n",
    "global_stocks = stock_re.merge(stock_pat, on=[\"Country\", \"Year\"], how=\"outer\") #merge on outer otherwise it will keep only common keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.3 Stocks x trade level (% GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports = wb_wdi.loc[wb_wdi[\"Series Name\"]==\"Imports of goods and services (% of GDP)\", :].copy()\n",
    "imports.drop(\"Series Name\", axis=1, inplace=True)\n",
    "imports.rename(columns={\"value\":\"imports_gdp\", \"Country\":\"Reporter\"}, inplace=True)\n",
    "\n",
    "df_tech = df_tech.merge(imports, on=[\"Reporter\", \"Year\"], how=\"left\")\n",
    "df_tech[\"patents_ds_imp_imp\"] = df_tech.patents_ds_imp*df_tech.imports_gdp\n",
    "df_tech[\"capacity_ws_ds_imp_imp\"] = df_tech.capacity_ws_ds_imp*df_tech.imports_gdp\n",
    "\n",
    "df_tech.drop(\"imports_gdp\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.4 Leakage risk index (foreign policy stringency)\n",
    "- Arguably, can be measured with respect to imports and exports. For now, focus on imports. But one could imagine building a variable that captures the policy stringency of trade competitors, as in Simmons and Elkins.\n",
    "- Question: the leakage risk should be measured with regard to the entire set of policies targeting the power sector, not policy by policy...?! ==> construct a composite index similar to Botta & Kozluk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary measures (all policies)\n",
    "# (capturing only legally binding FiTs/targets): either policy is in place or not\n",
    "\n",
    "co2_int_power = pd.read_csv(path_root+\"/IEA/IEA_GHG_int_elec.csv\")\n",
    "co2_int_power = co2_int_power.loc[(co2_int_power.PRODUCT==\"ADPRODUCT001\") & (co2_int_power.FLOW==\"ADFLOW001\"), [\"Country\", \"Time\", \"Value\"]]\n",
    "co2_int_power = co2_int_power.loc[~(co2_int_power.Country.str.match(\"Memo|OECD|Non|World|Other\")), :]\n",
    "co2_int_power = co2_int_power.replace(to_replace=map_iea_wb)\n",
    "\n",
    "df_leak = policies[[\"Country\", \"Year\"]]\n",
    "\n",
    "for mat in [tradeflows_share[\"IMP\"]]:\n",
    "    dyad_mat_leak = mat.merge(co2_int_power, left_on=[\"Partner\", \"Year\"], right_on=[\"Country\", \"Time\"], how='left')\n",
    "    dyad_mat_leak = dyad_mat_leak.merge(policies, left_on=[\"Partner\", \"Year\"], right_on=[\"Country\", \"Year\"], how='left')\n",
    "    dyad_mat_leak = dyad_mat_leak.drop([\"Country_x\", \"Country_y\"], axis=1)\n",
    "    dyad_mat_leak.rename(columns={\"Reporter\":\"Country\"}, inplace=True)\n",
    "    \n",
    "    for var in policy_indicators:\n",
    "        weights = list(mat.columns)[4:]\n",
    "    \n",
    "        for col in weights:\n",
    "            dyad_mat_leak[var+\"_\"+\"leakage_index\"] = dyad_mat_leak[col]*dyad_mat_leak.Value*dyad_mat_leak[var]\n",
    "\n",
    "    dyad_mat_leak.drop(policy_indicators+weights+[\"Time\", \"Value\"], axis=1, inplace=True)\n",
    "    dyad_mat_leak = dyad_mat_leak.groupby([\"Country\", \"Year\"]).sum()\n",
    "    dyad_mat_leak.reset_index(inplace=True)\n",
    "    \n",
    "    df_leak = df_leak.merge(dyad_mat_leak, on=[\"Country\", \"Year\"], how=\"left\")\n",
    "    \n",
    "# Continuous measures\n",
    "# Carbon pricing mechanisms: carbon price\n",
    "# RPS - % of RE to be procured (OECD, environmental policy stringency index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.4. Mean global policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Continuous measures\n",
    "#Share of country's power sector CO2 emissions in World total power sector emissions\n",
    "em_power = pd.read_csv(\"/Users/GD/OneDrive - rff/Documents/Research/projects/ecp/source_data/ghg_emissions/estimated-reported/national/IEA/detailed_figures/agg_product/iea_aggprod_WBnames.csv\")\n",
    "em_power = em_power.loc[(em_power.Flow==\"ABFLOW003\") & (em_power.Product==\"Total\"), ['Country', 'Year', 'Flow', 'Product', 'CO2_Emissions']]\n",
    "em_power = em_power.drop([\"Product\"], axis=1)\n",
    "               \n",
    "em_power = em_power.replace(to_replace=map_iea_wb)\n",
    "\n",
    "em_power_wld = em_power.loc[em_power.Country==\"World\", :]\n",
    "em_power_wld = em_power_wld.rename(columns={\"CO2_Emissions\":\"CO2_Emissions_wld\"})\n",
    "em_power_wld = em_power_wld.drop([\"Country\"], axis=1)\n",
    "em_power = em_power.merge(em_power_wld, on=[\"Year\", \"Flow\"], how='left')\n",
    "em_power = em_power.loc[em_power.Country!=\"World\", :]\n",
    "\n",
    "em_power.loc[:, \"CO2_share\"] = em_power.loc[:, \"CO2_Emissions\"]/em_power.loc[:, \"CO2_Emissions_wld\"]\n",
    "\n",
    "em_power = em_power.drop([\"Flow\", \"CO2_Emissions\", \"CO2_Emissions_wld\"], axis=1)\n",
    "\n",
    "free_riding = em_power.merge(policies, on=[\"Country\", \"Year\"], how='right')\n",
    "\n",
    "for col in policy_indicators:\n",
    "    free_riding.loc[:, col+\"_share\"] = free_riding.loc[:, col]*free_riding.loc[:, \"CO2_share\"]\n",
    "\n",
    "free_riding = free_riding.drop(policy_indicators+[\"CO2_share\"], axis=1)\n",
    "free_riding = free_riding.groupby([\"Year\"]).sum()\n",
    "free_riding = free_riding.reset_index()                \n",
    "\n",
    "temp = policies[[\"Country\", \"Year\"]].merge(free_riding, on=[\"Year\"], how='left')\n",
    "free_riding = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-f723ff9cd19d>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mean_policy_power[col] = np.nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Binary measures (all policies)\n",
    "# Total number of active schemes / total number of countries (Simmons & Elkins) - excluding own policy (different from Simons & Elkins)\n",
    "\n",
    "mean_policy_power = policies[[\"Country\", \"Year\"]]\n",
    "col_names = [\"tax_avg\", \"ets_avg\", \"fit_avg\", \"rps_avg\", \"pricing_avg\", \"techpol_avg\"]\n",
    "\n",
    "for col in col_names:\n",
    "    mean_policy_power[col] = np.nan\n",
    "\n",
    "for ctry in policies.Country.unique():\n",
    "    temp = policies.loc[policies.Country!=ctry, :].copy()\n",
    "    temp = temp.groupby(\"Year\").mean()\n",
    "    \n",
    "    i = 0\n",
    "    for col in col_names:\n",
    "        mean_policy_power.loc[mean_policy_power.Country==ctry, col] = np.array(temp.iloc[:, i])\n",
    "        i += 1\n",
    "\n",
    "mean_policy_power[\"clim_pol_avg\"] = (mean_policy_power.tax_avg+mean_policy_power.ets_avg+mean_policy_power.fit_avg+mean_policy_power.rps_avg)/4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Control mechanisms\n",
    "The ECP jurisdiction-year pairs should serve as reference for the panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domestic knowledge stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patents_cum - calculated above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Largest innovators identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inno_2016 = patents_cum.loc[(patents_cum.Year==2016)]\n",
    "\n",
    "#number of countries in sample\n",
    "#len(inno_2016)\n",
    "\n",
    "#inno_2016[pd.qcut(inno_2016['patents_ds'], 10, labels=range(10)).eq(9)]\n",
    "inno_2016_quant = inno_2016.loc[inno_2016['patents_ds'] > inno_2016['patents_ds'].quantile(0.9), :]\n",
    "\n",
    "inno_ctries = inno_2016_quant.Country.unique()\n",
    "\n",
    "inno_bin = policies[[\"Country\", \"Year\"]].copy()\n",
    "inno_bin[\"innovator\"] = 0\n",
    "inno_bin.loc[inno_bin.Country.isin(inno_ctries), \"innovator\"] = 1\n",
    "\n",
    "#inno_2016_quant.sort_values(by=\"patents_ds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Economic conditions (World Bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4163: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().drop(\n"
     ]
    }
   ],
   "source": [
    "#Pricing binary variables\n",
    "\n",
    "#WB WDI\n",
    "\n",
    "wb_wdi = pd.read_csv(path_root+\"/WB_WDI/WB_data.csv\", encoding='latin-1')\n",
    "wb_wdi.drop([\"Country Code\", \"Series Code\"], axis=1, inplace=True)\n",
    "\n",
    "wb_wdi = wb_wdi.melt(id_vars=[\"Country Name\", \"Series Name\"])\n",
    "\n",
    "wb_wdi.rename(columns={\"Country Name\":\"Country\", \"variable\":\"Year\"}, inplace=True)\n",
    "\n",
    "wb_wdi[\"Year\"] = wb_wdi[\"Year\"].apply(lambda x: x[:4])\n",
    "wb_wdi[\"Year\"] = wb_wdi[\"Year\"].astype(int)\n",
    "\n",
    "series_form = [\"imports\", \"exports\", \"co2_tot\", \"co2_pc\", \"gdp_pc_ppp\",\n",
    "               \"elec_coal\", \"elec_gas\", \"elec_oil\", \"pop_tot\"]\n",
    "\n",
    "wb_form = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for series in wb_wdi[\"Series Name\"].unique():\n",
    "    if i == 0:\n",
    "        temp = wb_wdi[wb_wdi[\"Series Name\"]==series].copy()\n",
    "        temp.drop(\"Series Name\", axis=1, inplace=True)\n",
    "        temp = temp.rename(columns={\"value\":series_form[i]})\n",
    "\n",
    "        wb_form = temp\n",
    "        \n",
    "    elif i > 0:\n",
    "        temp = wb_wdi[wb_wdi[\"Series Name\"]==series]\n",
    "        temp.drop(\"Series Name\", axis=1, inplace=True)\n",
    "        temp = temp.rename(columns={\"value\":series_form[i]})\n",
    "\n",
    "        wb_form = wb_form.merge(temp, on=[\"Country\", \"Year\"])\n",
    "\n",
    "    i += 1\n",
    "\n",
    "#drop some WDI variables not needed\n",
    "wb_form.drop([\"elec_coal\", \"elec_gas\", \"elec_oil\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power sector characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia = pd.read_csv(path_root+\"/EIA/EIA_FF_gen.csv\", skiprows=[0], encoding='latin-1')\n",
    "co2_em_elec = pd.read_csv(path_root+\"/IEA/IEA_CO2_elec.csv\", encoding=\"latin-1\")\n",
    "\n",
    "co2_em_elec = co2_em_elec[[\"Country\", \"TIME\", \"Value\"]]\n",
    "co2_em_elec.rename(columns={\"TIME\":\"Year\", \"Value\":\"co2em\"}, inplace=True)\n",
    "\n",
    "#EIA\n",
    "eia_ctry = np.array(eia.loc[eia.API.isna(), \"Unnamed: 1\"])\n",
    "\n",
    "eia_tot = pd.DataFrame(eia.loc[eia[\"Unnamed: 1\"]==\"    Generation (billion kWh)\", :])\n",
    "eia_tot[\"Country\"] = eia_ctry\n",
    "eia_tot.drop([\"API\", \"Unnamed: 1\"], axis=1, inplace=True)\n",
    "eia_tot = eia_tot.melt(id_vars=\"Country\")\n",
    "eia_tot.rename(columns={\"variable\":\"Year\", \"value\":\"Tot_gen_bkWh\"}, inplace=True)\n",
    "\n",
    "eia_ff = pd.DataFrame(eia.loc[eia[\"Unnamed: 1\"]==\"        Fossil fuels (billion kWh)\", :])\n",
    "eia_ff[\"Country\"] = eia_ctry\n",
    "eia_ff.drop([\"API\", \"Unnamed: 1\"], axis=1, inplace=True)\n",
    "eia_ff = eia_ff.melt(id_vars=\"Country\")\n",
    "eia_ff.rename(columns={\"variable\":\"Year\", \"value\":\"FF_gen_bkWh\"}, inplace=True)\n",
    "\n",
    "eia_form = eia_ff.merge(eia_tot, on=[\"Country\", \"Year\"])\n",
    "\n",
    "eia_wb_map = {'The Bahamas': 'Bahamas, The', 'Brunei': 'Brunei Darussalam', 'Burma': 'Myanmar', 'Congo-Brazzaville': 'Congo, Rep.',\n",
    "              'Congo-Kinshasa': 'Congo, Dem. Rep.', 'Côte d\\x92Ivoire': \"Cote d'Ivoire\", 'Egypt': 'Egypt, Arab Rep.',\n",
    "              'Hong Kong': 'Hong Kong SAR, China', 'Iran': 'Iran, Islamic Rep.', 'Kyrgyzstan': 'Kyrgyz Republic', 'Laos': 'Lao PDR',\n",
    "              'Macau': 'Macao SAR, China', 'North Korea': 'Korea, Dem. Rep.', 'Russia': 'Russian Federation',\n",
    "              'Saint Kitts and Nevis': 'St. Kitts and Nevis', 'Saint Lucia': 'St. Lucia',\n",
    "              'Saint Vincent/Grenadines': 'St. Vincent and the Grenadines', 'Slovakia': 'Slovak Republic', 'South Korea': 'Korea, Rep.',\n",
    "              'Syria': 'Syrian Arab Republic', 'Venezuela': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.'}\n",
    "\n",
    "iea_wb_map = {'Bolivarian Republic of Venezuela': 'Venezuela, RB', \"Côte d'Ivoire\": \"Cote d'Ivoire\", 'Egypt': 'Egypt, Arab Rep.',\n",
    "              \"Democratic People's Republic of Korea\": 'Korea, Dem. Rep.', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.',\n",
    "              'Hong Kong (China)': 'Hong Kong SAR, China', 'Islamic Republic of Iran': 'Iran, Islamic Rep.', 'Korea': 'Korea, Rep.',\n",
    "              'Kyrgyzstan': 'Kyrgyz Republic', \"Lao People's Democratic Republic\": 'Lao PDR', 'Myanmar': 'Myanmar',\n",
    "              \"People's Republic of China\": 'China', 'Plurinational State of Bolivia': 'Bolivia', 'Republic of Moldova': 'Moldova',\n",
    "              'Republic of North Macedonia': 'North Macedonia', 'Republic of the Congo': 'Congo, Rep.',\n",
    "              'United Republic of Tanzania': 'Tanzania', 'Viet Nam': 'Vietnam', 'Yemen': 'Yemen, Rep.'}\n",
    "\n",
    "eia_form.replace(to_replace=eia_wb_map, inplace=True)\n",
    "co2_em_elec.replace(to_replace=iea_wb_map, inplace=True)\n",
    "\n",
    "eia_form[\"Year\"] = eia_form[\"Year\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "power = eia_form.merge(co2_em_elec, on=[\"Country\", \"Year\"], how=\"left\")\n",
    "\n",
    "power.loc[power.FF_gen_bkWh==\"--\", \"FF_gen_bkWh\"] = np.nan\n",
    "power.loc[power.Tot_gen_bkWh==\"--\", \"Tot_gen_bkWh\"] = np.nan\n",
    "\n",
    "power[\"FF_gen_bkWh\"] = power[\"FF_gen_bkWh\"].astype(float)\n",
    "power[\"Tot_gen_bkWh\"] = power[\"Tot_gen_bkWh\"].astype(float)\n",
    "power[\"ff_perc_tot\"] = power.FF_gen_bkWh/power.Tot_gen_bkWh\n",
    "power[\"co2_int\"] = power.co2em/power.Tot_gen_bkWh\n",
    "\n",
    "power[\"ffperctot_co2int\"] = power.ff_perc_tot*power.co2_int\n",
    "\n",
    "power.drop([\"FF_gen_bkWh\", \"Tot_gen_bkWh\", \"co2em\"], axis=1, inplace=True)\n",
    "\n",
    "power = power.loc[(power.Year>=1990) & (power.Year<=2016), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Institutions (Varieties of Democracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VDEM\n",
    "vdem = pd.read_csv('/Users/gd/OneDrive - rff/Documents/Research/resources/Data/Datasets/VDem/Country_Year_V-Dem_Core_CSV_v10/V-Dem-CY-Core-v10.csv')\n",
    "vdem = vdem[[\"country_name\", \"year\", 'v2x_polyarchy', 'v2x_libdem']]\n",
    "\n",
    "vdem.rename(columns={\"country_name\":\"Country\", \"year\":\"Year\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdem_wb_map = {'Burma/Myanmar': 'Myanmar', 'Cape Verde': 'Cabo Verde', 'Democratic Republic of the Congo': 'Congo, Dem. Rep.',\n",
    "               'Egypt': 'Egypt, Arab Rep.', 'Hong Kong': 'Hong Kong SAR, China', 'Iran': 'Iran, Islamic Rep.', 'Ivory Coast': \"Cote d'Ivoire\",\n",
    "               'Kyrgyzstan': 'Kyrgyz Republic', 'Laos': 'Lao PDR', 'North Korea': 'Korea, Dem. Rep.', 'Republic of the Congo': 'Congo, Rep.',\n",
    "               'Russia': 'Russian Federation', 'Slovakia': 'Slovak Republic', 'South Korea': 'Korea, Rep.', 'Syria': 'Syrian Arab Republic',\n",
    "               'The Gambia': 'Gambia, The', 'United States of America': 'United States', 'Venezuela': 'Venezuela, RB', 'Yemen': 'Yemen, Rep.'}\n",
    "\n",
    "vdem.replace(to_replace=vdem_wb_map, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = policies.copy()\n",
    "duration = duration[[\"Country\", \"Year\"]]\n",
    "\n",
    "for ctry in duration.Country.unique():\n",
    "    i = 1 #start at one because some hazard models in the lifelines library can't deal with non-positive duration values\n",
    "    for yr in duration.Year.unique():\n",
    "        duration.loc[(duration.Country==ctry) & (duration.Year==yr), \"Duration\"] = i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CO2_share', 'tax_share', 'ets_share', 'fit_share', 'rps_share',\n",
       "       'pricing_share', 'techpol_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_riding.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Country'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-df77e9fc4cea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Reporter\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Country\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Year\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   7944\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7946\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m   7947\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7948\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m ) -> \"DataFrame\":\n\u001b[0;32m---> 74\u001b[0;31m     op = _MergeOperation(\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_rkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Country'"
     ]
    }
   ],
   "source": [
    "db = policies.copy()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for data in [df_policy, df_tech, patents_cum, df_leak, free_riding, global_stocks, power, inno_bin, duration, wb_form, power, vdem]:\n",
    "    print(i)\n",
    "    data.rename(columns={\"Reporter\":\"Country\"}, inplace=True)\n",
    "    db = db.merge(data, on=[\"Country\", \"Year\"], how='left')\n",
    "    i+=1\n",
    "    \n",
    "#identifier column\n",
    "db['UniqueID'] = db['Country'].astype(str)+'_'+db['Year'].astype(str)\n",
    "\n",
    "#keeping only observations between 1990 and 2016 (last year for which patent data is available)\n",
    "db = db.loc[(db['Year'] >= 1990) & (db['Year'] <= 2016), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of continuous, non-interacted, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_main = db.copy()\n",
    "\n",
    "norm_var = ['gdp_pc_ppp', 'pop_tot', 'co2_int', 'ff_perc_tot', 'v2x_polyarchy']\n",
    "\n",
    "for col in norm_var:\n",
    "    db_main[col+\"_sc\"] = db_main[col]/db_main[col].mean()*10-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of patents variables and renewable capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_col = ['patents_ds_imp', 'patents_ds_imp_imp', 'patents_ds_exp', 'patents_ds_global', 'patents_ds_eu', 'patents_ds']\n",
    "\n",
    "for col in scale_col:\n",
    "    db_main[col] = db_main[col]/100\n",
    "\n",
    "for col in [\"capacity_ds_global\"]:\n",
    "    db_main[col] = db_main[col]/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full dataset (for probit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctry_drop = [\"Andorra\", \"Djibouti\", \"Ethiopia\", \"Fiji\", \"Korea, Dem. Rep.\", \"Lesotho\", \"Liechtenstein\", \"Monaco\", \"Puerto Rico\", \n",
    "             \"Papua New Guinea\", \"San Marino\", \"Serbia\", \"Montenegro\", \"Somalia\", \"South Sudan\", \"Suriname\", \"West Bank and Gaza\", \"Sao Tome and Principe\",\n",
    "             \"St. Kitts and Nevis\", \"St. Lucia\", \"St. Vincent and the Grenadines\", \"Timor-Leste\", \"Trinidad and Tobago\"]\n",
    "\n",
    "db_main = db_main.loc[~db_main.Country.isin(ctry_drop), :]\n",
    "\n",
    "#drop NA values\n",
    "db_main.dropna(subset=['tax', 'ets', 'fit', 'rps', 'pricing', 'techpol','tax_imp', 'ets_imp', 'fit_imp', 'rps_imp', 'pricing_imp', 'techpol_imp',\n",
    "                       'tax_impexp', 'ets_impexp', 'fit_impexp', 'rps_impexp', 'pricing_impexp', 'techpol_impexp',\n",
    "                       'tax_comlang_off', 'tax_comrelig', 'tax_comcol', 'tax_col45', 'tax_fta_hmr',\n",
    "                       'ets_comlang_off', 'ets_comrelig', 'ets_comcol', 'ets_col45', 'ets_fta_hmr', \n",
    "                       'fit_comlang_off', 'fit_comrelig', 'fit_comcol', 'fit_col45', 'fit_fta_hmr',\n",
    "                       'rps_comlang_off', 'rps_comrelig', 'rps_comcol', 'rps_col45', 'rps_fta_hmr',\n",
    "                       'pricing_comlang_off', 'pricing_comrelig', 'pricing_comcol', 'pricing_col45', 'pricing_fta_hmr',\n",
    "                       'techpol_comlang_off', 'techpol_comrelig', 'techpol_comcol', 'techpol_col45', 'techpol_fta_hmr',\n",
    "                       'patents_ds_imp', 'capacity_ws_ds_imp', 'patents_ds_imp_imp', 'capacity_ws_ds_imp_imp',\n",
    "                       #'tax_avg', 'ets_avg', 'fit_avg', 'rps_avg',\n",
    "                       'imports', 'gdp_pc_ppp', 'ff_perc_tot', 'ffperctot_co2int', 'v2x_polyarchy'], inplace=True) #, \n",
    "\n",
    "#restrict to 101 countries for which we have patent info\n",
    "#pat_ctry = ks_pat.Country.unique()\n",
    "#db_main = db_main.loc[db_main.Country.isin(pat_ctry), :]\n",
    "\n",
    "db_main.to_csv(path_root_ii+'/data/dataset/CPdiff_dataset_v2.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets for hazard regressions\n",
    "#### Tax, ETS, FiT, RPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset where only the first observation post implementation is kept\n",
    "\n",
    "i = 0\n",
    "\n",
    "dummies = ['tax', 'ets', 'fit', 'rps', 'pricing', 'techpol']\n",
    "\n",
    "for policy in dummies:\n",
    "    db_hazard_0 = db_main.loc[db[policy]==0, :].copy()\n",
    "    db_hazard_1 = db_main.loc[db[policy]==1, :].copy()\n",
    "\n",
    "    #Sorting dataframe - VERY important that the dataframe be sorted by:'Jurisdiction' then 'Year' otherwise the next\n",
    "    #command will simply keep the first entry which might well be a different year than the first in which a scheme was\n",
    "    #introduced\n",
    "    db_hazard_1.sort_values(by=['Country','Year'],inplace=True)\n",
    "    db_hazard_1.drop_duplicates(subset=['Country', policy],keep='first',inplace=True)\n",
    "\n",
    "    db_hazard = pd.concat([db_hazard_0, db_hazard_1])\n",
    "\n",
    "    #drop dummy of other policies\n",
    "    drop_list = [x for x in dummies if x != policy]\n",
    "    db_hazard.drop(drop_list, axis=1, inplace=True)\n",
    "    \n",
    "    #also drop the covariates that pertain to other policies\n",
    "    col_names = [\"tax\", \"ets\", \"fit\", \"rps\", \"pricing\", \"techpol\"]\n",
    "    discard = []\n",
    "    \n",
    "    drop_col = col_names\n",
    "    drop_col.pop(i)\n",
    "    \n",
    "    for col in drop_col:\n",
    "        temp_list = [x for x in db_hazard.columns if x.startswith(col) == True]\n",
    "        discard.extend(temp_list)\n",
    "    \n",
    "    covariates = [x for x in db_hazard.columns if x not in discard]\n",
    "    \n",
    "    db_hazard = db_hazard[covariates]\n",
    "    \n",
    "    #rename columns to remove reference to policy\n",
    "    for col in [x for x in db_hazard.columns if x.startswith(dummies[i]) == True]:\n",
    "        db_hazard.rename(columns={col:\"policy_\"+col[4:]}, inplace=True)\n",
    "        \n",
    "    \n",
    "    db_hazard.to_csv(path_root_ii+'/data/dataset/CPdiff_hz_'+dummies[i]+'_v2.csv',index=None)\n",
    "    i +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
